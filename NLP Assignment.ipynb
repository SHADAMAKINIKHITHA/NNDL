{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMCjQ8AM1yaPA9onxP0SpK+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SHADAMAKINIKHITHA/NNDL/blob/main/NLP%20Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bsu4wIGUxpS6"
      },
      "source": [
        "s ='''Are you fascinated by the amount of text data available on the internet?\n",
        "Are you looking for ways to work with this text data but aren’t sure where to begin?\n",
        "Machines, after all, recognize numbers, not the letters of our language. \n",
        "And that can be a tricky landscape to navigate in machine learning.\n",
        "Solving an NLP problem is a multi-stage process. \n",
        "We need to clean the unstructured text data first before we can even think about getting to the modeling stage. \n",
        "Cleaning the data consists of a few key steps'''"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYGSU214yMhZ",
        "outputId": "119cae88-0d20-48ba-86a7-0ff682fcbae0"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "#a = nltk.tokenize.sent_tokenize(s)\n",
        "from nltk.tokenize import sent_tokenize\n",
        "token_text = sent_tokenize(s)\n",
        "#print(\"\\nSentence-tokenized copy in a list:\")\n",
        "#print(token_text)\n",
        "#print(\"\\nRead the list:\")\n",
        "for s in token_text:\n",
        "    print(s)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Are you fascinated by the amount of text data available on the internet?\n",
            "Are you looking for ways to work with this text data but aren’t sure where to begin?\n",
            "Machines, after all, recognize numbers, not the letters of our language.\n",
            "And that can be a tricky landscape to navigate in machine learning.\n",
            "Solving an NLP problem is a multi-stage process.\n",
            "We need to clean the unstructured text data first before we can even think about getting to the modeling stage.\n",
            "Cleaning the data consists of a few key steps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP7kGPf5yuol",
        "outputId": "a544eaa2-a9b9-45e6-a783-c1e659144f4b"
      },
      "source": [
        "print([i for item in token_text for i in item.split()])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Are', 'you', 'fascinated', 'by', 'the', 'amount', 'of', 'text', 'data', 'available', 'on', 'the', 'internet?', 'Are', 'you', 'looking', 'for', 'ways', 'to', 'work', 'with', 'this', 'text', 'data', 'but', 'aren’t', 'sure', 'where', 'to', 'begin?', 'Machines,', 'after', 'all,', 'recognize', 'numbers,', 'not', 'the', 'letters', 'of', 'our', 'language.', 'And', 'that', 'can', 'be', 'a', 'tricky', 'landscape', 'to', 'navigate', 'in', 'machine', 'learning.', 'Solving', 'an', 'NLP', 'problem', 'is', 'a', 'multi-stage', 'process.', 'We', 'need', 'to', 'clean', 'the', 'unstructured', 'text', 'data', 'first', 'before', 'we', 'can', 'even', 'think', 'about', 'getting', 'to', 'the', 'modeling', 'stage.', 'Cleaning', 'the', 'data', 'consists', 'of', 'a', 'few', 'key', 'steps']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK1D8aOuyw6w"
      },
      "source": [
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMnEMFdMy2Fh",
        "outputId": "1a717ee9-4ba5-4236-9b23-7cd1e256646b"
      },
      "source": [
        "porter = PorterStemmer()\n",
        "lancaster=LancasterStemmer()\n",
        "print(\"Porter Stemmer\")\n",
        "print(porter.stem(\"cats\"))\n",
        "print(porter.stem(\"trouble\"))\n",
        "print(porter.stem(\"troubling\"))\n",
        "print(porter.stem(\"troubled\"))\n",
        "print(porter.stem(\"having\"))\n",
        "print(porter.stem(\"Corriendo\"))\n",
        "print(porter.stem(\"at\"))\n",
        "print(porter.stem(\"was\"))\n",
        "\n",
        "\n",
        "print(\"Lancaster Stemmer\")\n",
        "print(lancaster.stem(\"cats\"))\n",
        "print(lancaster.stem(\"trouble\"))\n",
        "print(lancaster.stem(\"troubling\"))\n",
        "print(lancaster.stem(\"troubled\"))\n",
        "print(lancaster.stem(\"having\"))\n",
        "print(lancaster.stem(\"Corriendo\"))\n",
        "print(lancaster.stem(\"at\"))\n",
        "print(lancaster.stem(\"was\"))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer\n",
            "cat\n",
            "troubl\n",
            "troubl\n",
            "troubl\n",
            "have\n",
            "corriendo\n",
            "at\n",
            "wa\n",
            "Lancaster Stemmer\n",
            "cat\n",
            "troubl\n",
            "troubl\n",
            "troubl\n",
            "hav\n",
            "corriendo\n",
            "at\n",
            "was\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0s64skluy87M",
        "outputId": "5898704d-7a39-4146-ba72-a53bbb2a71db"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8m97ET68zCfy",
        "outputId": "a3e2d281-bf8c-44ec-db81-6bd15f2f0ce1"
      },
      "source": [
        "words = [\"cats\",\"trouble\",\"troubling\",\"troubled\",\"having\",\"Corriendo\",\"at\",\"was\"]\n",
        "lemmatizer = WordNetLemmatizer()   \n",
        "\n",
        "#an instance of Word Net Lemmatizer\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words] \n",
        "print(\"The lemmatized words: \", lemmatized_words) \n",
        "#prints the lemmatized words\n",
        "lemmatized_words_pos = [lemmatizer.lemmatize(word, pos = \"v\") for word in words]\n",
        "print(\"The lemmatized words using a POS tag: \", lemmatized_words_pos) \n",
        "#prints POS tagged lemmatized words"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The lemmatized words:  ['cat', 'trouble', 'troubling', 'troubled', 'having', 'Corriendo', 'at', 'wa']\n",
            "The lemmatized words using a POS tag:  ['cat', 'trouble', 'trouble', 'trouble', 'have', 'Corriendo', 'at', 'be']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zctsrM6CzHsE"
      },
      "source": [
        "s1=\"The NLTK library is one of the oldest and most commonly used Python libraries for Natural Language Processing. NLTK supports stop word removal, and you can find the list of stop words in the corpus module. To remove stop words from a sentence, you can divide your text into words and then remove the word if it exits in the list of stop words provided by NLTK.\"\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVByyXLRzOWE",
        "outputId": "a9364bb6-57c4-4e30-8541-c15094030db8"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CnPD7XozYQ5",
        "outputId": "8c4902eb-70bf-4ba2-bbae-c1daca421fe9"
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "word_tokens = word_tokenize(s1)\n",
        "print(word_tokens)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'NLTK', 'library', 'is', 'one', 'of', 'the', 'oldest', 'and', 'most', 'commonly', 'used', 'Python', 'libraries', 'for', 'Natural', 'Language', 'Processing', '.', 'NLTK', 'supports', 'stop', 'word', 'removal', ',', 'and', 'you', 'can', 'find', 'the', 'list', 'of', 'stop', 'words', 'in', 'the', 'corpus', 'module', '.', 'To', 'remove', 'stop', 'words', 'from', 'a', 'sentence', ',', 'you', 'can', 'divide', 'your', 'text', 'into', 'words', 'and', 'then', 'remove', 'the', 'word', 'if', 'it', 'exits', 'in', 'the', 'list', 'of', 'stop', 'words', 'provided', 'by', 'NLTK', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0epO55-0IPH",
        "outputId": "f67f5f97-d117-4576-c03d-54fe8149ea47"
      },
      "source": [
        "\n",
        "for i in word_tokens:\n",
        "    if i in stop_words:\n",
        "        print(i)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is\n",
            "of\n",
            "the\n",
            "and\n",
            "most\n",
            "for\n",
            "and\n",
            "you\n",
            "can\n",
            "the\n",
            "of\n",
            "in\n",
            "the\n",
            "from\n",
            "a\n",
            "you\n",
            "can\n",
            "your\n",
            "into\n",
            "and\n",
            "then\n",
            "the\n",
            "if\n",
            "it\n",
            "in\n",
            "the\n",
            "of\n",
            "by\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AATlR65K0PYQ"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import webtext\n",
        "from nltk.probability import FreqDist"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEajBBF90SEf",
        "outputId": "38996057-d922-405f-99f6-131c142c4204"
      },
      "source": [
        "nltk.download('webtext')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/webtext.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6n0-8E80VlI"
      },
      "source": [
        "data_analysis = nltk.FreqDist(word_tokens)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMy5x3OO0Zeb",
        "outputId": "122708a8-cdfd-4ced-bae4-af510df49eb1"
      },
      "source": [
        "filter_words = dict([(m, n) for m, n in data_analysis.items() if len(m) > 3])\n",
        "\n",
        "for key in sorted(filter_words):\n",
        "    print(\"%s: %s\" % (key, filter_words[key]))\n",
        " \n",
        "data_analysis = nltk.FreqDist(filter_words)\n",
        " \n",
        "#data_analysis.plot(25, cumulative=False)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language: 1\n",
            "NLTK: 3\n",
            "Natural: 1\n",
            "Processing: 1\n",
            "Python: 1\n",
            "commonly: 1\n",
            "corpus: 1\n",
            "divide: 1\n",
            "exits: 1\n",
            "find: 1\n",
            "from: 1\n",
            "into: 1\n",
            "libraries: 1\n",
            "library: 1\n",
            "list: 2\n",
            "module: 1\n",
            "most: 1\n",
            "oldest: 1\n",
            "provided: 1\n",
            "removal: 1\n",
            "remove: 2\n",
            "sentence: 1\n",
            "stop: 4\n",
            "supports: 1\n",
            "text: 1\n",
            "then: 1\n",
            "used: 1\n",
            "word: 2\n",
            "words: 4\n",
            "your: 1\n"
          ]
        }
      ]
    }
  ]
}